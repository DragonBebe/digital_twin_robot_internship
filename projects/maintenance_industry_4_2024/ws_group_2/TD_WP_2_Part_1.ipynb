{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exective summary of Work Package 2\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this WP, you will work on a given training dataset. Your goal is to develop a fault detection model using the classification algorithms learnt in the class, in order to achieve best F1 score.\n",
    "\n",
    "## Tasks\n",
    "\n",
    "- Task 1: Develop a fault detection model using the unsupervised learning algorithms learnt in the class, in order to achieve best F1 score.\n",
    "- Task 2: With the help of the supporting script, develop a cross-validation scheme to test the performance of the developed classification algorithms.\n",
    "- Task 3: Develop a fault detection model using the classification algorithms learnt in the class, in order to achieve best F1 score.\n",
    "\n",
    "## Delierables\n",
    "\n",
    "- A Jupyter notebook reporting the process and results of the above tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before starting, please:\n",
    "- Fetch the most up-to-date version of the github repository.\n",
    "- Create a new branch with your name, based on the \"main\" branch and switch to your own branch.\n",
    "- Copy this notebook to the work space of your group, and rename it to TD_WP_2_Your name.ipynb\n",
    "- After finishing this task, push your changes to the github repository of your group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Unsupervised learning approaches\n",
    "\n",
    "## Implement the statistical testing approach for fault detection\n",
    "\n",
    "In this exercise, we interpret the statistical testing approach for fault detection. The basic idea of statistical testing approach is that we fit a multi-dimensitional distribution to the observation data under normal working condition. Then, when a new data point arrives, we design a hypothesis test to see whether the new data point is consistent with the distribution. If the new data point is consistent with the distribution, we can conclude that the fault is not due to the faulty component.\n",
    "\n",
    "The benefit of this approach is that, to design the detection algrothim, we do not need failed data. Also, the computational time is short as all we need is just to compute the pdf and compare it to a threshold.\n",
    "\n",
    "In this exercise, you need to:\n",
    "- Fit a multi-dimensitional distribution to the training dataset (all normal samples).\n",
    "- Design a fault detection algorithm based on the fitted distribution to detect faulty components.\n",
    "\n",
    "The following block defines a few functions that you can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "\n",
    "def estimateGaussian(X):\n",
    "    '''Given X, this function estimates the parameter of a multivariate Gaussian distribution.'''\n",
    "    mu = np.mean(X, axis=0)\n",
    "    sigma2 = np.var(X, axis=0)\n",
    "    return mu, sigma2\n",
    "\n",
    "\n",
    "def classify(X, distribution, log_epsilon=-50):\n",
    "    '''Given X, this function classifies each sample in X based on the multivariate Gaussian distribution. \n",
    "       The decision rule is: if the log pdf is less than log_epsilon, we predict 1, as the sample is unlikely to be from the distribution, which represents normal operation.\n",
    "    '''\n",
    "    p = distribution.logpdf(X)\n",
    "    predictions = (p < log_epsilon).astype(int)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use the dataset `20240105_164214` as training dataset, as all the samples in this dataset are normal operation. We will use the dataset `20240325_155003` as testing dataset. Let us try to predict the state of motor 1. For this, we first extract the position, temperature and voltage of motor 1 as features (you can change the features if you want). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../supporting_scripts/WP_1')\n",
    "\n",
    "from utility import read_all_csvs_one_test\n",
    "import pandas as pd\n",
    "\n",
    "# Specify path to the dictionary.\n",
    "base_dictionary = '../dataset/training_data/'\n",
    "dictionary_name = '20240105_164214'\n",
    "path = base_dictionary + dictionary_name\n",
    "\n",
    "# Read the data.\n",
    "df_data = read_all_csvs_one_test(path, dictionary_name)\n",
    "\n",
    "# Get the features\n",
    "X_train = df_data[['data_motor_1_position', 'data_motor_1_temperature', 'data_motor_1_voltage']]\n",
    "\n",
    "# We do the same to get the test dataset.\n",
    "dictionary_name = '20240325_155003'\n",
    "path = base_dictionary + dictionary_name\n",
    "\n",
    "# Read the data.\n",
    "df_data = read_all_csvs_one_test(path, dictionary_name)\n",
    "\n",
    "# Get the features\n",
    "X_test = df_data[['data_motor_1_position', 'data_motor_1_temperature', 'data_motor_1_voltage']]\n",
    "y_test = df_data['data_motor_1_label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please design your algorithm below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.3252769385699899\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# First, we need to fit a MVN distribution to the normal samples.\n",
    "mu, sigma2 = estimateGaussian(X_train)\n",
    "\n",
    "# Construct a multivariate Gaussian distribution to represent normal operation.\n",
    "distribution = multivariate_normal(mean=mu, cov=np.diag(sigma2))\n",
    "\n",
    "# Now, let's try to predict the labels of the test set X_test.\n",
    "y_pred = classify(X_test, distribution)\n",
    "\n",
    "# Calculate accuracy of the prediction.\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1-score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussions:**\n",
    "- Can you please try to improve the performance of this approach?\n",
    "    - For example, by normalizating the data?\n",
    "    - By smoothing the data?\n",
    "    - By reducing feature number?\n",
    "    - etc.\n",
    "- The parameter log_epsilon defines the threshold we use for making classification. What happens if you change it?\n",
    "- Could you discuss how we should get the best value for this parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['data_motor_1_position', 'data_motor_1_temperature',\n",
      "       'data_motor_1_voltage'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardization : \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "cols = X_train.columns\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X_train))\n",
    "X_scaled.columns = cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_motor_1_position</th>\n",
       "      <th>data_motor_1_temperature</th>\n",
       "      <th>data_motor_1_voltage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.652000e+03</td>\n",
       "      <td>3.652000e+03</td>\n",
       "      <td>3.652000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.111436e-01</td>\n",
       "      <td>-3.113002e-16</td>\n",
       "      <td>9.214486e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.110375e-16</td>\n",
       "      <td>1.000137e+00</td>\n",
       "      <td>1.000137e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.111436e-01</td>\n",
       "      <td>-2.687957e+00</td>\n",
       "      <td>-2.233978e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.111436e-01</td>\n",
       "      <td>-8.184004e-01</td>\n",
       "      <td>-7.114257e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.111436e-01</td>\n",
       "      <td>4.279708e-01</td>\n",
       "      <td>-4.123530e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.111436e-01</td>\n",
       "      <td>1.051156e+00</td>\n",
       "      <td>1.083011e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.111436e-01</td>\n",
       "      <td>1.051156e+00</td>\n",
       "      <td>2.469621e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       data_motor_1_position  data_motor_1_temperature  data_motor_1_voltage\n",
       "count           3.652000e+03              3.652000e+03          3.652000e+03\n",
       "mean            4.111436e-01             -3.113002e-16          9.214486e-15\n",
       "std             1.110375e-16              1.000137e+00          1.000137e+00\n",
       "min             4.111436e-01             -2.687957e+00         -2.233978e+00\n",
       "25%             4.111436e-01             -8.184004e-01         -7.114257e-01\n",
       "50%             4.111436e-01              4.279708e-01         -4.123530e-01\n",
       "75%             4.111436e-01              1.051156e+00          1.083011e+00\n",
       "max             4.111436e-01              1.051156e+00          2.469621e+00"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Deal with outliers : \n",
    "for column in X_scaled.columns : \n",
    "    Q1 = X_scaled[column].quantile(0.25)\n",
    "    Q3 = X_scaled[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    X_scaled[column]= np.where(X_scaled[column] > upper_bound, upper_bound, X_scaled[column])\n",
    "    X_scaled[column] = np.where(X_scaled[column] < lower_bound, lower_bound, X_scaled[column])\n",
    "    \n",
    "X_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_motor_1_position</th>\n",
       "      <th>data_motor_1_temperature</th>\n",
       "      <th>data_motor_1_voltage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86</td>\n",
       "      <td>42</td>\n",
       "      <td>7223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86</td>\n",
       "      <td>42</td>\n",
       "      <td>7214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>86</td>\n",
       "      <td>42</td>\n",
       "      <td>7137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>86</td>\n",
       "      <td>42</td>\n",
       "      <td>7135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86</td>\n",
       "      <td>42</td>\n",
       "      <td>7212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3647</th>\n",
       "      <td>86</td>\n",
       "      <td>48</td>\n",
       "      <td>7144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3648</th>\n",
       "      <td>86</td>\n",
       "      <td>48</td>\n",
       "      <td>7148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3649</th>\n",
       "      <td>86</td>\n",
       "      <td>48</td>\n",
       "      <td>7241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3650</th>\n",
       "      <td>86</td>\n",
       "      <td>48</td>\n",
       "      <td>7228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3651</th>\n",
       "      <td>86</td>\n",
       "      <td>48</td>\n",
       "      <td>7130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3652 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      data_motor_1_position  data_motor_1_temperature  data_motor_1_voltage\n",
       "0                        86                        42                  7223\n",
       "1                        86                        42                  7214\n",
       "2                        86                        42                  7137\n",
       "3                        86                        42                  7135\n",
       "4                        86                        42                  7212\n",
       "...                     ...                       ...                   ...\n",
       "3647                     86                        48                  7144\n",
       "3648                     86                        48                  7148\n",
       "3649                     86                        48                  7241\n",
       "3650                     86                        48                  7228\n",
       "3651                     86                        48                  7130\n",
       "\n",
       "[3652 rows x 3 columns]"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = X_train.columns\n",
    "X_train2 = pd.DataFrame(scaler.fit_transform(X_scaled))\n",
    "X_train2.columns = cols\n",
    "X_train2 = X_train2.drop(\"data_motor_1_position\", axis = 1)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (6652,3) (2,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[362], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m distribution2 \u001b[38;5;241m=\u001b[39m multivariate_normal(mean\u001b[38;5;241m=\u001b[39mmu, cov\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mdiag(sigma2), allow_singular\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Now, let's try to predict the labels of the test set X_test.\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m y_pred_preprocess \u001b[38;5;241m=\u001b[39m \u001b[43mclassify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistribution2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_epsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2.5\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Calculate accuracy of the prediction.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred_preprocess)\n",
      "Cell \u001b[1;32mIn[355], line 17\u001b[0m, in \u001b[0;36mclassify\u001b[1;34m(X, distribution, log_epsilon)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclassify\u001b[39m(X, distribution, log_epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m50\u001b[39m):\n\u001b[0;32m     14\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Given X, this function classifies each sample in X based on the multivariate Gaussian distribution. \u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m       The decision rule is: if the log pdf is less than log_epsilon, we predict 1, as the sample is unlikely to be from the distribution, which represents normal operation.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogpdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m (p \u001b[38;5;241m<\u001b[39m log_epsilon)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "File \u001b[1;32mc:\\Users\\Come\\Documents\\Scolarité\\CentraleSup\\2A\\Industry4.0\\digital_twin_robot_group2\\.venv\\lib\\site-packages\\scipy\\stats\\_multivariate.py:917\u001b[0m, in \u001b[0;36mmultivariate_normal_frozen.logpdf\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogpdf\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    916\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dist\u001b[38;5;241m.\u001b[39m_process_quantiles(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim)\n\u001b[1;32m--> 917\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_logpdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcov_object\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcov_object\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim):\n\u001b[0;32m    919\u001b[0m         out_of_bounds \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcov_object\u001b[38;5;241m.\u001b[39m_support_mask(x\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean)\n",
      "File \u001b[1;32mc:\\Users\\Come\\Documents\\Scolarité\\CentraleSup\\2A\\Industry4.0\\digital_twin_robot_group2\\.venv\\lib\\site-packages\\scipy\\stats\\_multivariate.py:530\u001b[0m, in \u001b[0;36mmultivariate_normal_gen._logpdf\u001b[1;34m(self, x, mean, cov_object)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Log of the multivariate normal probability density function.\u001b[39;00m\n\u001b[0;32m    512\u001b[0m \n\u001b[0;32m    513\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    527\u001b[0m \n\u001b[0;32m    528\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    529\u001b[0m log_det_cov, rank \u001b[38;5;241m=\u001b[39m cov_object\u001b[38;5;241m.\u001b[39mlog_pdet, cov_object\u001b[38;5;241m.\u001b[39mrank\n\u001b[1;32m--> 530\u001b[0m dev \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dev\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    532\u001b[0m     log_det_cov \u001b[38;5;241m=\u001b[39m log_det_cov[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, np\u001b[38;5;241m.\u001b[39mnewaxis]\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (6652,3) (2,) "
     ]
    }
   ],
   "source": [
    "# First, we need to fit a MVN distribution to the normal samples.\n",
    "mu, sigma2 = estimateGaussian(X_train2)\n",
    "\n",
    "# Construct a multivariate Gaussian distribution to represent normal operation.\n",
    "distribution2 = multivariate_normal(mean=mu, cov=np.diag(sigma2), allow_singular=True)\n",
    "\n",
    "# Now, let's try to predict the labels of the test set X_test.\n",
    "y_pred_preprocess = classify(X_test, distribution2, log_epsilon=-2.5*10**7)\n",
    "\n",
    "# Calculate accuracy of the prediction.\n",
    "accuracy = accuracy_score(y_test, y_pred_preprocess)\n",
    "f1 = f1_score(y_test, y_pred_preprocess)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1-score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7500000000.0 0.0\n",
      "-8750000000.0 0.0\n",
      "-9375000000.0 0.0\n",
      "-9687500000.0 0.0\n",
      "-9843750000.0 0.0\n",
      "-9921875000.0 0.0\n",
      "-9960937500.0 0.0\n",
      "-9980468750.0 0.0\n",
      "-9990234375.0 0.0\n",
      "-9995117187.5 0.0\n",
      "-9997558593.75 0.0\n",
      "-9998779296.875 0.0\n",
      "-9999389648.4375 0.0\n",
      "-9999694824.21875 0.0\n",
      "-9999847412.109375 0.0\n",
      "-9999923706.054688 0.0\n",
      "-9999961853.027344 0.0\n",
      "-9999980926.513672 0.0\n",
      "-9999990463.256836 0.0\n",
      "-9999995231.628418 0.0\n",
      "-9999997615.814209 0.0\n",
      "-9999998807.907104 0.0\n",
      "-9999999403.953552 0.0\n",
      "-9999999701.976776 0.0\n",
      "-9999999850.988388 0.0\n",
      "-9999999925.494194 0.0\n",
      "-9999999962.747097 0.0\n",
      "-9999999981.373549 0.0\n",
      "-9999999990.686775 0.0\n",
      "-9999999995.343388 0.0\n",
      "-9999999997.671694 0.0\n",
      "-9999999998.835846 0.0\n",
      "-9999999999.417923 0.0\n",
      "-9999999999.708961 0.0\n",
      "-9999999999.85448 0.0\n",
      "-9999999999.92724 0.0\n",
      "-9999999999.96362 0.0\n",
      "Optimal log_epsilon: -9999999999.96362\n",
      "Corresponding accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def dichotomy_algorithm(X_test, y_test, distribution, log_epsilon_min, log_epsilon_max, tol=0.1):\n",
    "    # Define a tolerance level to stop the iteration\n",
    "    diff = log_epsilon_max - log_epsilon_min\n",
    "    eps = (log_epsilon_max + log_epsilon_min) / 2\n",
    "\n",
    "    # Iteratively adjust log_epsilon until convergence\n",
    "    while diff > tol:       \n",
    "        y_pred_eps = classify(X_test, distribution, log_epsilon = eps)\n",
    "        accuracy = f1_score(y_test, y_pred_eps) \n",
    "\n",
    "        y_pred_ = classify(X_test, distribution, log_epsilon = log_epsilon_min)\n",
    "        accuracy_min = f1_score(y_test, y_pred_preprocess)\n",
    "        \n",
    "        # Update log_epsilon_min or log_epsilon_max based on the accuracy\n",
    "        if accuracy_min < accuracy:\n",
    "            log_epsilon_min = eps\n",
    "        else:\n",
    "            log_epsilon_max = eps\n",
    "        \n",
    "        eps = (log_epsilon_max + log_epsilon_min) / 2\n",
    "        # Update the difference for convergence check\n",
    "        diff = log_epsilon_max - log_epsilon_min\n",
    "        print(eps, accuracy)\n",
    "    \n",
    "    # Return the optimal log_epsilon_mid\n",
    "    return eps, accuracy\n",
    "\n",
    "# Set initial values for log_epsilon_min and log_epsilon_max\n",
    "log_epsilon_min = -10**10  # Initial lower bound\n",
    "log_epsilon_max = 0   # Initial upper bound\n",
    "\n",
    "# Call the dichotomy_algorithm function\n",
    "optimal_log_epsilon, optimal_accuracy = dichotomy_algorithm(X_test, y_test, distribution, log_epsilon_min, log_epsilon_max)\n",
    "\n",
    "# Print the optimal values\n",
    "print(\"Optimal log_epsilon:\", optimal_log_epsilon)\n",
    "print(\"Corresponding accuracy:\", optimal_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dichotomy for epsilon : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.06584485868911606\n"
     ]
    }
   ],
   "source": [
    "print(y_pred.mean())\n",
    "print(y_pred_preprocess.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local outiler factor (LOF)\n",
    "\n",
    "The local outlier factor (LOF) algorithm computes the local density deviation of a given data point with respect to its neighbors. It considers as outliers the samples that have a substantially lower density than their neighbors. You can easiliy implement LOF in scikit-learn ([tutorial](https://www.datatechnotes.com/2020/04/anomaly-detection-with-local-outlier-factor-in-python.html)).\n",
    "\n",
    "Please implement local outlier factor (LOF) algorithm on the dataset of `20240325_155003`. You can try first to detect the failure of motor 1 using this model. Please calculate the accuracy score of your prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
