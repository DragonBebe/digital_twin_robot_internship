{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exective summary of Work Package 1\n",
    "\n",
    "## Objectives\n",
    "\n",
    "This work package aims at developing methods for cleaning and preprocessing the data, and select the most relevant features for further model development.\n",
    "\n",
    "## Tasks\n",
    "\n",
    "- Task 1: Read and understand the data\n",
    "  - Understand how to use the provided utility function to read the data\n",
    "  - Understand the data structure and the meaning of each column\n",
    "- Task 2: Clean and preprocess the data\n",
    "  - Explore the quality of the data\n",
    "  - Develop methods to remove the outlier and clean the data\n",
    "- Task 3: Feature engineering\n",
    "  - Visualize the data\n",
    "  - Develop methods to select the most relevant features\n",
    "\n",
    "## Delierables\n",
    "\n",
    "- A Jupyter notebook reporting the process and results of the above tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before starting, please:\n",
    "- Fetch the most up-to-date version of the github repository.\n",
    "- Create a new branch called \"WP2_TD_Lect_5_YourName\", based on the branch \"zhiguo_dev\" and switch to it.\n",
    "- Copy the notebook WP2_DATA-DRIVEN FAULT DETECTION/support/WP2_TD_Lect_5 to WP2_DATA-DRIVEN FAULT DETECTION/TD_5/, and rename it to TD_Lect_5_YourName.ipynb\n",
    "- After finishing this task, push your changes to your github repository.\n",
    "- Submit a pull request to the \"zhiguo_dev\" branch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 Read and understand the data\n",
    "\n",
    "## Sub-task 1: Read the data from the different csv files and store it in a dataframe. \n",
    "\n",
    "The training data is stored in the following path: `projects\\maintenance_industry_4_2024\\dataset\\training_data`. In this folder, there are some subfolders. Each subfolder contains the data of one test. The name of the subfolder tells you the time when the test is performed. Inside each subfolder, there are six csv files, each file corresponds to the data from one motor. The file `Test condition.xlsx` tells you the test condition for each test.\n",
    "\n",
    "In `utility.py`, there are supporting functions `read_all_csvs_one_test` and `read_all_test_data_from_path`. Please have a look at the demo in `demo_read_data_and_preprocess.ipynb`, and use these supporting functions to read the data.\n",
    "\n",
    "\n",
    "**Your task:**\n",
    "\n",
    "- Create a dataframe to store data from all the test.\n",
    "- Write a paragraph to explain the structure of the resulted dataframe (what does each column represent?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31135 entries, 0 to 31134\n",
      "Data columns (total 26 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   time                      31135 non-null  float64\n",
      " 1   data_motor_1_position     31135 non-null  int64  \n",
      " 2   data_motor_1_temperature  31135 non-null  int64  \n",
      " 3   data_motor_1_voltage      31135 non-null  int64  \n",
      " 4   data_motor_1_label        31135 non-null  int64  \n",
      " 5   data_motor_2_position     31135 non-null  int64  \n",
      " 6   data_motor_2_temperature  31135 non-null  int64  \n",
      " 7   data_motor_2_voltage      31135 non-null  int64  \n",
      " 8   data_motor_2_label        31135 non-null  int64  \n",
      " 9   data_motor_3_position     31135 non-null  int64  \n",
      " 10  data_motor_3_temperature  31135 non-null  int64  \n",
      " 11  data_motor_3_voltage      31135 non-null  int64  \n",
      " 12  data_motor_3_label        31135 non-null  int64  \n",
      " 13  data_motor_4_position     31135 non-null  int64  \n",
      " 14  data_motor_4_temperature  31135 non-null  int64  \n",
      " 15  data_motor_4_voltage      31135 non-null  int64  \n",
      " 16  data_motor_4_label        31135 non-null  int64  \n",
      " 17  data_motor_5_position     31135 non-null  int64  \n",
      " 18  data_motor_5_temperature  31135 non-null  int64  \n",
      " 19  data_motor_5_voltage      31135 non-null  int64  \n",
      " 20  data_motor_5_label        31135 non-null  int64  \n",
      " 21  data_motor_6_position     31135 non-null  int64  \n",
      " 22  data_motor_6_temperature  31135 non-null  int64  \n",
      " 23  data_motor_6_voltage      31135 non-null  int64  \n",
      " 24  data_motor_6_label        31135 non-null  int64  \n",
      " 25  test_condition            31135 non-null  object \n",
      "dtypes: float64(1), int64(24), object(1)\n",
      "memory usage: 6.2+ MB\n"
     ]
    }
   ],
   "source": [
    "from utility import read_all_csvs_one_test, read_all_test_data_from_path\n",
    "\n",
    "base_dictionary = '../../dataset/training_data/'\n",
    "df_data = read_all_test_data_from_path(base_dictionary, is_plot=False)\n",
    "\n",
    "df_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please explain the structure (what does each column represent) of the resulted dataframe here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataframe, we have 26 features, the time where the features are measured, and for each of the 6 motors we have 4 parameters : the position, temperature, voltage and label. The label is 0 if the motor is normal and 1 if there is a problem. Finally we have the test condition which describe the global robot's status at each time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-task 2: Data visualization\n",
    "\n",
    "Visualize the data. By examing the data, you can answer the following questions:\n",
    "- Do we have a lot noise in the features? Do we need to smooth the data?\n",
    "- Do we need to scale the features?\n",
    "- Do we have a lot of outliers? Do we need to remove them?\n",
    "- Do we have a lot of missing values? Do we need to fill them?\n",
    "- In general, do you discover some patterns regarding how temperature of the motor changes?\n",
    "- If we compare the normal data (label = 0) with the failures (label=1), could you see some difference in the pattern of the temperature?\n",
    "\n",
    "**Submit your summary below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "S = 0\n",
    "for cols in df_data.columns:\n",
    "    S += sum(np.asarray(df_data[cols].isnull()))\n",
    "    \n",
    "S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is some noise for continuous features like voltage, thus we need to apply a moving average algorithm to smooth the data.\n",
    "- Scaling the features is always usefull so the have the same weights in classification algorithms. Here we have outliers so a standardization is relevant.\n",
    "- Like we said, we have lots of outliers especially for the position's features, we have to remove them otherwise it could mislead our classification algorithms\n",
    "- We don't have any missing values like shows the previous cell\n",
    "- It seems like hot temperature and motor's issue are highly correlated\n",
    "- While the robot status is normal, the temperature usually do not increase quickly\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-task 3 Explore the distribution of each feature through histogram and box plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\maxde\\Documents\\CS\\2A\\4.0\\digital_twin_robot_group2\\projects\\maintenance_industry_4_2024\\supporting_scripts\\WP_1\\TD_WP_1_Max.ipynb Cell 11\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/maxde/Documents/CS/2A/4.0/digital_twin_robot_group2/projects/maintenance_industry_4_2024/supporting_scripts/WP_1/TD_WP_1_Max.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/maxde/Documents/CS/2A/4.0/digital_twin_robot_group2/projects/maintenance_industry_4_2024/supporting_scripts/WP_1/TD_WP_1_Max.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/maxde/Documents/CS/2A/4.0/digital_twin_robot_group2/projects/maintenance_industry_4_2024/supporting_scripts/WP_1/TD_WP_1_Max.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m L \u001b[39m=\u001b[39m df_data\u001b[39m.\u001b[39mcolumns\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/maxde/Documents/CS/2A/4.0/digital_twin_robot_group2/projects/maintenance_industry_4_2024/supporting_scripts/WP_1/TD_WP_1_Max.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m15\u001b[39m, \u001b[39m3\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/maxde/Documents/CS/2A/4.0/digital_twin_robot_group2/projects/maintenance_industry_4_2024/supporting_scripts/WP_1/TD_WP_1_Max.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m4\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_data' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "L = df_data.columns\n",
    "\n",
    "plt.figure(figsize=(15, 3))\n",
    "\n",
    "for k in range(4):\n",
    "    plt.subplot(1, 4, k+1)\n",
    "    sns.histplot(data=df_data, x=L[k], kde=True)\n",
    "    print(k)\n",
    "    \n",
    "### Pas afficher le temps et le dernier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-task 4 Use PCA to visulize the data in a 2-d plane.\n",
    "\n",
    "Normally, before any further analysis, we try to plot all the features directly in a 2-d plane. This is because the 2-d plane is a very simple representation of the data and allows us to visually inspect the patterns of the data. A normal conclusion we can make is whether the data are linear seperable or not, i.e., if you can simply fit a straight line to seperate postitive and negative classes. However, if the data is high-dimensional, it is difficult to visualize it in a 2-d plane. In this case, we can use PCA to reduce the dimensionality of the data and then plot it in a 2-d plane.\n",
    "\n",
    "Below is a code for performing PCA and plotting the data in a 2-d plane. What can you see from the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Separate the features (X) and the target variable (y)\n",
    "X = df.drop(['label', 'sequence_idx', 'time'], axis=1)\n",
    "y = df['label']\n",
    "\n",
    "# Perform PCA to reduce the dimensionality to 2 dimensions\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[y == 0, 0], X_pca[y == 0, 1], color='red', marker='^', alpha=.5, label='Class 0')\n",
    "plt.scatter(X_pca[y == 1, 0], X_pca[y == 1, 1], color='blue', marker='o', alpha=.5, label='Class 1')\n",
    "plt.title('2D PCA Result')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above analysis, we did not perform data normalization/standardization. Could you try to do that and see if the results change? You can use the follwing code for standardization:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "```\n",
    "\n",
    "And this for normalization:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 Data cleaning and preprocessing\n",
    "\n",
    "## Task 1: Normalize the dataset.\n",
    "\n",
    "Define your strategy to normalize the dataset and implement the data. Please pay attention to the difference between fit_transform(), fit(), and transform()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Define a strategy to remove outliers.\n",
    "\n",
    "Explain how you remove the outliers and implement your approach here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Define a strategy to smooth the data.\n",
    "\n",
    "Explain how do you smooth the data and implement your approach here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 Feature engineering\n",
    "\n",
    "## Task 1: Use violin plot to explore the significance of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Use correlation coefficient matrix to explore the correlation among the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion of feature engineering\n",
    "\n",
    "**Write your conclusion of feature engineering here.** For example, which features are most relevant? Which features are not relevant? Which features are redundant? Which features are irrelevant? Eventually, what are your decisions regarding which features to be used?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-env",
   "language": "python",
   "name": "dl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
